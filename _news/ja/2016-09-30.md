---
layout: simple
title: 'DDBJデータ解析チャレンジ受賞者の発表'
category: news
db:
  - ddbj

tags:
  - お知らせ

date: 2016-09-30T09:10:30+09:00
lang: ja
---

<div class="mainbox clearfix">
    <div class="float_right">2016年9月30日</div>
</div>
<div class="mainbox clearfix">
    <div class="float_right">DDBJデータ解析チャレンジ実施委員会</div>
</div>2016年7月6日から8月31日に<a href="/news/ja/2016-06-27_2.html">DDBJデータ解析チャレンジ</a>を実施しました。DDBJデータ解析チャレンジでは、DDBJ保有のビッグデータである塩基配列公開データベースを用いて、チャレンジ課題の機械学習モデルの精度を競います。 構築モデルは京都大学の<a href="http://universityofbigdata.net/competition/5749873794088960" target="_blank">ビッグデータ大学</a>に投稿します。今回「DNA配列からのクロマチン特徴予測」のチャレンジ課題について、38名が参加して、延べ360回のモデル投稿がありました。モデル精度が上位1位～3位の最優秀賞・優秀賞・優良賞と、参加学生中で1位の学生賞の対象者を発表致します。また最優秀賞の構築モデルの概要を記載します。

<h4>DDBJデータ解析チャレンジ2016 入賞者</h4>

<table>
    <tbody>
        <tr>
            <td style="border-color: #999999;">最優秀賞1st Prize of DDBJ Challenge Awards 2016</td>
            <td style="border-color: #999999;">株式会社 情報数理バイオ　研究開発部 ライフサイエンスグループ望月 正弘</td>
        </tr>
        <tr>
            <td style="border-color: #999999;">優秀賞2nd Prize of DDBJ Challenge Awards 2016</td>
            <td style="border-color: #999999;">国立研究開発法人 理化学研究所 情報基盤センター バイオインフォマティクス研究開発ユニット松本 拡高(代表※)、尾崎 遼(※)※チームとして2名で参加</td>
        </tr>
        <tr>
            <td style="border-color: #999999;">優良賞3rd Prize of DDBJ Challenge Awards 2016</td>
            <td style="border-color: #999999;">ビッツ株式会社岡山 利次</td>
        </tr>
        <tr>
            <td style="border-color: #999999;">学生賞Student Prize of DDBJ Challenge Awards 2016</td>
            <td style="border-color: #999999;">東京大学大学院 情報理工学系研究科 修士課程1年加藤 卓也</td>
        </tr>
    </tbody>
</table> 

<h4>最優秀賞モデルの概要</h4>望月氏のモデルは、<span style="color: #d85c3e;">Extremely Randomized Trees</span>(ERT, 参考文献1) と <span style="color: #d85c3e;">Convolutional Neural Network</span>(CNN, 参考文献2) の2種類の分類器を基盤として、<span style="color: #d85c3e;">Stacked Generalization</span>(Stacking, 参考文献3) アンサンブル学習法で精度向上を図っています。特徴量は、チャレンジのクエリ配列だけでなく<span style="color: #d85c3e;">外部特徴量(ゲノム座標、遺伝子注釈情報)を組み入れています</span>。1つ目のERTモデルはゲノム座標が特徴量でn(配列数) x m(染色体数)の行列です。ゲノム座標はクエリ配列をシロイヌナズナTAIR10ゲノム参照配列(参考文献 4)にアライメントして得ます。このERTモデルをGenomic Coordinates Based Model(GCBM)とします。2つ目のCNNモデルの特徴量は、クエリ配列と遺伝子注釈情報(TAIRからGFFファイルをダウンロード)です。Figure 1の様にforward/reverse strand別で遺伝子注釈情報を組み込んでいます。遺伝子注釈情報は定量値で、定義は次式になります。<img src="{{ site.baseurl }}/assets/images/news/ddbjchal_2016_teigi.png">変数rは減衰率で、変数dは遺伝子の1塩基目からの距離です。変数rが0なら特徴量は1になり、遺伝子中に対象塩基が含まれる事を表します。変数rが0より大きい時は、遺伝子開始塩基からの勾配値が与えられます。このCNNモデルをGene Annotated Sequences Based Model(GASBM)とします。<span class="font-bold">Figure1: Structure of the neural network of GASBM</span><img src="{{ site.baseurl }}/assets/images/news/ddbjchal_2016_figure1.png">Figure2はベンチマーク結果です。ERT-GCBMモデルとCNN-GASBMモデルのパラメータ値などの詳細は、他の受賞者モデルも含めた全体解説(データクレンジング、特徴量選択、モデル訓練と予測法)と共に、報告論文で公開する予定です。<span class="font-bold">Figure2: Benchmark result of models</span><img src="{{ site.baseurl }}/assets/images/news/ddbjchal_2016_figure2.png"><span class="font-bold">Reference</span>[1]　Geurts, P., Ernst, D. &amp; Wehenkel, L. Extremely randomized trees. Machine Learning 63, 3-42 (2006).[2]　LeCun, Yann, et al. “Gradient-based learning applied to document recognition.” Proceedings of the IEEE 86.11 (1998): 2278-2324.[3]　Wolpert, D. H. Stacked generalization. Neural Networks 5, 241-259 (1992).[4]　<a href="https://www.arabidopsis.org/" target="_blank">https://www.arabidopsis.org/</a>

<h4>受賞モデルの精度結果</h4>

<table>
    <tbody>
        <tr>
            <th style="border-color: #999999;">DDBJ Challenge Award</th>
            <th style="border-color: #999999;">AUC</th>
            <th>Model Design</th>
            <th> Tool Version</th>
        </tr>
        <tr>
            <td style="border-color: #999999;" align="center">1st Prize</td>
            <td style="border-color: #999999;">0.94564</td>
            <td>*2 Classifiers(Extremely Randomized Trees, CNN)*Ensemble Learning(Stacking)*External Data(Genomic Position, Gene Structure Annotation)</td>
            <td>python=3.5scikit-learn=0.17.1chainer=1.10.0</td>
        </tr>
        <tr>
            <td style="border-color: #999999;" align="center">2nd Prize</td>
            <td style="border-color: #999999;">0.89859</td>
            <td>*2 Classifiers(CNN, Product of Genomic Distance Decay Parameter and  Nearest Training Data Output)*Ensemble Learning(Averaged)*External Data(Genomic Position)</td>
            <td>julia=0.4.6python=2.7.10skflow(tensorflow=0.8.0)</td>
        </tr>
        <tr>
            <td style="border-color: #999999;" align="center">3rd Prize</td>
            <td style="border-color: #999999;">0.85428</td>
            <td>*7 Classifiers(<span class="st">Naive Bayes for Multivariate Bernoulli Models</span>, Logistic Regression, Random Forest, Gradient Boosting, <span class="st">Extremely Randomized Trees</span>, eXtreme Gradient Boosting, CNN)*Ensemble Learning (Stacking)</td>
            <td>python=2.7.11numpy=1.10.4scikit-learn=0.17chainer=1.11.0xgboost=0.4a30</td>
        </tr>
        <tr>
            <td style="border-color: #999999;" align="center">Student Prize</td>
            <td style="border-color: #999999;">0.84318</td>
            <td>*3 Classifiers(LeNet like CNN, DeepBind like CNN, Variable filter DeepBind like CNN)*Ensemble Learning(Soft Voting)</td>
            <td>python=2.7lasagne=0.2.dev1</td>
        </tr>
    </tbody>
</table> <span class="icon_d-triangle"> <span class="font-bold">お問い合わせ：</span><a href="/address.html#to-ddbj">DDBJへのお問い合わせ</a>の「DDBJデータ解析チャレンジ」からご連絡ください。</span>
